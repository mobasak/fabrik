# Windsurf Cascade Project Rules
# Symlink this file to project root: ln -s /opt/fabrik/windsurfrules .windsurfrules

## Environment Context

**Development:** WSL (Windows Subsystem for Linux)
**Production:** VPS via Coolify (Docker Compose deployments)
**SSH Target:** `vps` (configured in ~/.ssh/config)

### Key Principles

- **WSL is the dev factory** — All coding happens here
- **VPS is production** — Managed by Coolify, deployed via Docker Compose
- **Research before code** — Complete research phase before creating project folder
- **Deployment-ready from line 1** — Code must work in all target environments without changes
- **Docker-first development** — Search for existing containerized solutions before writing custom code

### Starting a New Project

When user says **"new project"**, **"start a project"**, or **"create [project-name]"**:

```bash
# 1. Scaffold the project structure
cd /opt/fabrik && source .venv/bin/activate
python -c "from fabrik.scaffold import create_project; create_project('PROJECT_NAME', 'DESCRIPTION')"

# 2. Result: /opt/PROJECT_NAME with:
#    - AGENTS.md (droid exec instructions)
#    - Dockerfile + compose.yaml (Docker-ready)
#    - src/main.py (FastAPI starter with /health)
#    - tests/test_health.py (basic test)
#    - requirements.txt
#    - docs/ structure
#    - .windsurfrules symlink
#    - Git initialized

# 3. Verify it works
cd /opt/PROJECT_NAME
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
pytest tests/
uvicorn src.main:app --reload --port 8000
```

**For SaaS/web apps:** Use `cp -r /opt/fabrik/templates/saas-skeleton /opt/PROJECT_NAME` instead.

### Docker-First Development

Before implementing custom solutions for common problems (notifications, PDF generation, search, browser automation, CMS, etc.), search for existing Docker images:

```bash
cd /opt/fabrik && source .venv/bin/activate
python scripts/container_images.py search <need>
python scripts/container_images.py check-arch <image:tag>  # ARM64 required for VPS1
python scripts/container_images.py trueforge list          # Supply-chain secure images
```

**Reference:** `/opt/fabrik/docs/reference/phase9.md` — Full acceleration map with project-specific recommendations.

### Project Types

**Not all projects need containers.** Decide at project start:

| Type | Container? | Deploy Method |
|------|------------|---------------|
| **Service** (API, web app, worker) | ✅ Yes | Dockerfile + compose.yaml → Coolify |
| **Library/CLI/Script** | ❌ No | rsync or pip install on VPS |

**Rule:** If it listens on a port OR runs 24/7 → needs container. Otherwise → no container.

For services: Dockerfile + compose.yaml required from day 1.
For libraries/tools: Just documentation structure, no Docker files.

### Container Base Image Standard

**Use Ubuntu/Debian bases, NOT Alpine:**

| Use Case | Base Image |
|----------|------------|
| Python apps | `python:3.12-slim-bookworm` |
| Node.js apps | `node:22-bookworm-slim` |
| General services | `debian:bookworm-slim` |
| Supply-chain secure | `oci.trueforge.org/tccr/ubuntu` |

**Why not Alpine:** glibc compatibility, pre-built Python wheels, familiar debugging tools, better ARM64 support.

### Target Deployment Environments

All code MUST work in these environments without modification:

| Environment | Database | Vector Store | Config Source |
|-------------|----------|--------------|---------------|
| **WSL (dev)** | PostgreSQL localhost | pgvector localhost | `.env` file |
| **VPS Docker** | postgres-main container | pgvector container | compose.yaml env |
| **Supabase** | Supabase PostgreSQL | Supabase pgvector | env vars |

**This means:**
- **NEVER hardcode `localhost`** — Use `os.getenv('DB_HOST', 'localhost')`
- **NEVER hardcode connection strings** — Build from env vars
- **NEVER use class-level config dicts** — Env vars aren't set at import time
- **Health checks MUST test actual dependencies** — Not just return `{"status": "ok"}`

### Fabrik Skills (Auto-Invoked by Droid)

Droids automatically invoke Fabrik skills when relevant keywords are detected:

| Skill | Auto-Triggers On |
|-------|------------------|
| `fabrik-saas-scaffold` | **"SaaS", "web app", "subscription app", "dashboard app"** |
| `fabrik-scaffold` | "new project", "create service" |
| `fabrik-docker` | "dockerfile", "compose", "deploy" |
| `fabrik-health-endpoint` | "health", "healthcheck" |
| `fabrik-config` | "config", "environment", "settings" |
| `fabrik-preflight` | "preflight", "deploy ready" |
| `fabrik-api-endpoint` | "endpoint", "route", "API" |
| `fabrik-watchdog` | "watchdog", "monitor", "auto-restart" |
| `fabrik-postgres` | "database", "postgres", "migration" |
| `fabrik-documentation` | "document", "docs", "readme", "update docs", "audit docs" |

### SaaS Projects (MANDATORY)

**When starting ANY SaaS, web app, or dashboard project, ALWAYS use the SaaS skeleton template:**

```bash
cp -r /opt/fabrik/templates/saas-skeleton /opt/<project-name>
cd /opt/<project-name>
npm install
cp .env.example .env
npm run dev
```

**Template includes:**
- Next.js 14 + TypeScript + Tailwind CSS
- Marketing pages (landing, pricing, FAQ, terms, privacy)
- App pages (dashboard, settings, job workflow)
- SSE streaming + ChatUI for droid exec integration
- Supabase-ready auth patterns

**Customize:** `lib/config/site.ts` for branding, `app/(marketing)/` for content.

**Location:** `~/.factory/skills/` — Skills ensure all Fabrik conventions are followed automatically.

### Cascade + droid exec

When Cascade drives droid exec (most common workflow):
- **Always use `--auto high`** — Cascade can't confirm interactive prompts
- **Break into small steps** — Smaller task = less risk, easier rollback
- **Best models score ~65% on terminal tasks** — Don't trust complex multi-step commands

**Factory settings:** `/home/ozgur/.factory/settings.json` — Check for model defaults, reasoning settings.

```bash
# BAD: Big task that can fail in many ways
droid exec --auto high "Set up entire Docker infrastructure with postgres, redis, nginx"

# GOOD: Small steps, each verifiable
droid exec --auto high "Create Dockerfile for Python app"
droid exec --auto high "Create compose.yaml with postgres service"
droid exec --auto high "Test docker compose up"
```

### droid exec Quick Reference

```bash
# Read-only analysis (safe default)
droid exec "Analyze this codebase"

# File edits
droid exec --auto low "Add comments to functions"

# Development (when YOU are in terminal)
droid exec --auto medium "Install deps and run tests"

# Cascade-driven (no interactive prompts)
droid exec --auto high "Fix, test, commit, push"

# Specification mode (plan before code)
droid exec --use-spec "Add authentication"

# Model + reasoning
droid exec -m gemini-3-flash-preview "Quick task"
droid exec -m gpt-5.1-codex-max -r high "Complex task"

# Output formats
droid exec -o json "task"          # Structured
droid exec -o stream-json "task"   # Real-time JSONL
```

**Key flags:** `--auto`, `--use-spec`, `-m`, `-r`, `-o`, `--cwd`, `-s`

**Model Management (Automated):**
```bash
./scripts/setup_model_updates.sh               # Enable daily auto-updates (cron)
python3 scripts/droid_model_updater.py         # Force update now
python3 scripts/droid_models.py stack-rank     # View current rankings
python3 scripts/droid_models.py recommend ci_cd # Get model for scenario
```
**Config:** `config/models.yaml` — Auto-updated from Factory docs daily

**Code Review (Dual-Model - ALWAYS use BOTH):**
```bash
# Get models dynamically from config (names change!)
python3 scripts/droid_models.py recommend code_review

# Run all models from config
for model in $(python3 -c "import yaml; c=yaml.safe_load(open('config/models.yaml')); print(' '.join(c['scenarios']['code_review']['models']))"); do
  droid exec -m "$model" "Review [files]..."
done
```

### Auto-Run Mode (Risk-Based Autonomy)

| Level | What Runs Automatically | Use Case |
|-------|------------------------|----------|
| **Default** | Read-only only | Safe exploration |
| **`--auto low`** | + File edits | Docs, formatting |
| **`--auto medium`** | + Reversible changes (installs, commits) | Dev work |
| **`--auto high`** | All non-blocked commands | CI/CD, deployments |

**Always blocked:** `rm -rf /`, `dd of=/dev/*`, command substitution `$(...)`, CLI security flags.

### Implementing Large Features (30+ files)

```bash
# 1. Master plan with spec mode
droid exec --use-spec "Create implementation plan for [feature]"
# Save as IMPLEMENTATION_PLAN.md

# 2. Phase by phase (fresh session each)
droid exec --auto medium "Implement Phase 1 per IMPLEMENTATION_PLAN.md"

# 3. Commit per phase
droid exec --auto medium "Commit Phase 1 with detailed message"
```

**Key:** Fresh session per phase, test after each, plan rollback.

### Batch Refactoring Scripts

Location: `scripts/droid/`

| Script | Purpose |
|--------|---------|
| `refactor-imports.sh` | Organize Python imports |
| `improve-errors.sh` | Improve error messages |
| `fix-lint.sh` | Fix lint violations with AI |

Usage: `DRY_RUN=true ./scripts/droid/refactor-imports.sh src`

### VPS Deployment (Coolify)

Install droid CLI on VPS host, apps call via subprocess.
For SaaS web apps: Use `--output-format stream-json` for SSE streaming.
See `droid-exec-usage.md` §25-26 for patterns.

### GitHub Actions Workflows

Location: `.github/workflows/`

| Workflow | Trigger |
|----------|---------|
| `droid-review.yml` | PR opened/updated |
| `update-docs.yml` | Push to main |
| `security-scanner.yml` | Weekly (Monday 9AM) |
| `daily-maintenance.yml` | Daily (3AM) |

**Setup:** Add `FACTORY_API_KEY` to repository secrets.

### Available Tools

**Factory.ai Droid (`droid exec`) is installed in this WSL environment.**

`droid exec` is a **batch-oriented, tool-gated execution engine**, intended for *controlled, cost-aware automation* — not ad-hoc browsing.

#### Primary use cases

- Batch research where **external evidence is required**
- Multi-pass pipelines:
  - Pass A: fact extraction / verification (minimal output)
  - Pass B: escalation for a small subset (bounded tools)
  - Pass C: final generation (no tools)
- Tasks that benefit from: tool gating, strict output schemas, caching and deduplication
- **Coding tasks** (analysis, refactor, test generation, dependency updates) — with edits/commands only when `--auto` enables them

#### Security model (secure-by-default)

- **Default mode is read-only/spec-mode** (analysis/planning only)
- Use `--auto low|medium|high` to allow edits/commands depending on risk tier:
  - `--auto low`: minimal autonomy, safest
  - `--auto medium`: moderate autonomy
  - `--auto high`: full autonomy (use with caution)
- Tool categories:
  - **read tools**: enabled for analysis (default safe)
  - **edit tools** (`Create/Edit/ApplyPatch`): enable only when changes are desired
  - **execute tools** (`Execute`): enable only when running commands is required
- Prefer explicit tool gating over blanket `--auto high`
- **Note:** Coding tasks may involve file edits and command execution only when the corresponding tools are enabled by autonomy AND permissions. Autonomy ≠ blanket permission — tools still matter.

#### Usage rules (IMPORTANT)

- **Do not use web tools by default**
- Prefer:
  - `--disabled-tools FetchUrl`
  - `--auto low` for non-research passes
- WebSearch should be:
  - ≤1 per item
  - used only when facts are not explicit in input data
- FetchUrl must be:
  - disabled by default
  - enabled only in an escalation pass for flagged items
- Separate **fact discovery** from **writing/expansion**
- Default to coarse answers (`nationwide`, `international`) unless evidence explicitly supports higher precision

#### When to suggest `droid exec`

- The user explicitly requests:
  - research with external evidence
  - batch processing with structured output
  - verification rather than summarization
  - large-scale refactors, test generation, dependency updates, or repo-wide standards enforcement
- Or when a multi-pass, cost-controlled workflow is beneficial

#### Before using

- Always consult: `/opt/fabrik/docs/reference/droid-exec-usage.md`
- **Ask permission before invoking `droid exec`** — do not auto-execute
- Suggest with reasoning: "This looks like a batch research task. I can use `droid exec` in a cost-gated, multi-pass mode. Should I proceed?"

### Pre-Project Phase

Before creating a project folder, complete research:
1. Create research document(s) using `/opt/fabrik/templates/scaffold/docs/RESEARCH_TEMPLATE.md`
2. Evaluate tools, APIs, costs
3. Document selected approach and justification
4. Only then: `fabrik new --scaffold <name> "<description>"`

---

## Project Setup Requirements

Every project MUST have:

### Configuration Files
- `.env.example` - Template with all env vars (never commit real .env)
- `config/` folder for yaml/json configs
- Document all env vars in docs/CONFIGURATION.md

### Credentials Management (MANDATORY)

**All API keys, tokens, and credentials MUST be stored in TWO places:**

1. **Project `.env`** — For local development and project-specific use
2. **Fabrik master `.env`** — Central credentials repository at `/opt/fabrik/.env`

**When adding new credentials:**
```bash
# 1. Add to project .env
echo "NEW_API_KEY=xyz123" >> /opt/myproject/.env

# 2. ALSO add to Fabrik master .env
echo "NEW_API_KEY=xyz123" >> /opt/fabrik/.env
```

**Why both?**
- Project `.env` — Used by the project locally
- Fabrik `.env` — Single source of truth for disaster recovery, new deployments, and cross-project access

**Fabrik .env structure:**
```
/opt/fabrik/.env contains:
- VPS/SSH credentials
- Coolify API token
- All external API keys (OpenAI, Anthropic, DeepL, etc.)
- All service credentials (Webshare, Anti-Captcha, etc.)
- Database credentials for all projects
- Email service credentials (Resend, SES)
- Backup credentials (B2)
```

**Before marking any task complete that involves new credentials:**
- [ ] Added to project `.env`
- [ ] Added to `/opt/fabrik/.env`
- [ ] Added to project `.env.example` (without real values)

### Password & Secret Generation Policy

**Use CSPRNG (Cryptographically Secure) for ALL passwords/secrets:**

| Requirement | Value |
|-------------|-------|
| Length | 32 chars (adapt down only if system limit) |
| Characters | `[a-zA-Z0-9]` (no special chars) |
| Generator | Python `secrets`, Node `crypto.randomBytes` |
| Truncation | NEVER truncate; generate correct length |

```python
import secrets, string
secret = ''.join(secrets.choice(string.ascii_letters + string.digits) for _ in range(32))
```

**Weak passwords like `postgres`, `admin`, `password123` are FORBIDDEN.**

### Fabrik Service Documentation Standard

**When deploying a service to Coolify under Fabrik, ALL of the following are MANDATORY:**

1. **Project README.md** must include:
   - "Integration from Other Services" section
   - URL table: WSL, VPS (container), External
   - Code example with env vars
   - Auth requirements (API key, etc.)

2. **Fabrik SERVICES.md** (`/opt/fabrik/docs/SERVICES.md`) must include:
   - Service in the VPS Services table
   - Service in the Service Reference table
   - Integration section with code example (if auth required)

3. **Credentials in BOTH .env files:**
   ```bash
   # Add to project .env
   echo "SERVICE_API_KEY=<key>" >> /opt/<service>/.env

   # ALSO add to Fabrik master .env
   echo "SERVICE_API_KEY=<key>" >> /opt/fabrik/.env
   ```

4. **Environment variable naming convention:**
   - URL: `<SERVICE>_URL` (e.g., `TRANSLATOR_URL`)
   - API Key: `<SERVICE>_API_KEY` (e.g., `TRANSLATOR_API_KEY`)

5. **Uptime Kuma monitoring:**
   - Add service to `/opt/fabrik/scripts/setup_uptime_kuma.py`
   - Run: `cd /opt/fabrik && source .venv/bin/activate && python scripts/setup_uptime_kuma.py`
   - Or use driver: `add_fabrik_service_to_monitoring("servicename", "vps1.ocoron.com", "/health")`

**Checklist before marking service deployment complete:**
- [ ] Project README has integration section
- [ ] Fabrik SERVICES.md updated
- [ ] Credentials in project `.env`
- [ ] Credentials in `/opt/fabrik/.env`
- [ ] Credentials in project `.env.example` (without real values)
- [ ] Health check documented and tested
- [ ] Service added to Uptime Kuma monitoring

### Database (if applicable)
- `docs/reference/database.md` - Schema documentation
- Migration files in `migrations/` or `alembic/`
- Always document schema changes in CHANGELOG.md

### Logging
- Use structured logging (JSON format for production)
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Separate log files: `app.log`, `error.log`
- Include: timestamp, level, module, message, context
- Never log secrets/passwords/tokens

### Project Structure (Code)
```
/opt/<project>/
├── src/ or <package_name>/    # Main source code
│   ├── __init__.py
│   ├── main.py                # Entry point
│   ├── config.py              # Config loading
│   ├── models/                # Data models
│   ├── services/              # Business logic
│   ├── api/                   # API endpoints
│   └── utils/                 # Helpers
├── tests/                     # Test files mirror src/
├── scripts/                   # Utility scripts
├── config/                    # Config files
├── data/                      # Persistent data files (gitignored)
├── .tmp/                      # Temporary files (gitignored, safe to delete)
├── .cache/                    # Cache files (gitignored, safe to delete)
├── output/                    # Generated outputs (gitignored unless needed)
├── logs/                      # Log files (gitignored)
└── docs/                      # Documentation
```

### FORBIDDEN: System Temp Directories

**NEVER use these directories for ANY data:**
- `/tmp/`
- `/var/tmp/`
- `tempfile.gettempdir()` or similar system temp functions
- Any path outside the project folder

**WHY:** Data in `/tmp` is shared across all projects and processes. It gets deleted on WSL restart or by other processes. This causes DATA LOSS.

**ALWAYS use project-local directories instead:**
```python
# WRONG - will lose data
import tempfile
temp_dir = tempfile.gettempdir()  # Returns /tmp

# CORRECT - data stays with project
from pathlib import Path
PROJECT_ROOT = Path(__file__).parent.parent
TEMP_DIR = PROJECT_ROOT / ".tmp"
TEMP_DIR.mkdir(exist_ok=True)
```

**Project-local directories:**
| Directory | Purpose | Gitignored | Safe to Delete |
|-----------|---------|------------|----------------|
| `.tmp/` | Temporary/scratch files | Yes | Yes |
| `.cache/` | Cached data (API responses, etc.) | Yes | Yes |
| `data/` | Persistent data files | Yes | No |
| `output/` | Generated outputs (reports, exports) | Usually | Depends |

## Agile Development Rules

### Before Writing Code
1. Understand the requirement fully - ask if unclear
2. Check if similar code exists - reuse don't duplicate
3. Plan the minimal solution first

### While Writing Code
- Small, focused commits - one thing per commit
- During git workflows, use sed/shell commands for edits instead of the edit tool to avoid file state conflicts.
- Write tests alongside code, not after
- Fail fast - validate inputs early
- Log decisions and errors, not just success

### Code Quality (Non-Negotiable)
- Type hints on all functions
- Docstrings on public functions
- No hardcoded values - use config/env
- Handle errors explicitly - no bare except
- Keep functions <50 lines, files <500 lines

### Review Checklist (Before Done)
- [ ] Works as expected (tested)
- [ ] Error cases handled
- [ ] Logged appropriately
- [ ] Config externalized
- [ ] Docs updated (ALWAYS - no code change without doc update)

## Documentation Structure

Follow the documentation standard at `/opt/fabrik/docs/DOCUMENTATION_STANDARD.md`.

Required files for every project:
- README.md (root) - Project overview
- CHANGELOG.md (root) - Version history
- tasks.md (root) - Development tracking (Phase > Task > Subtask)
- docs/README.md - Documentation index with structure map
- docs/QUICKSTART.md - Getting started
- docs/CONFIGURATION.md - Settings reference
- docs/TROUBLESHOOTING.md - Common issues
- docs/BUSINESS_MODEL.md - Monetization strategy (create early, research later)
- docs/DEPLOYMENT.md - Deployment configuration (ports, env vars, Docker)
- docs/SERVICES.md - Required services documentation (if project has services)

### Pre-Project Research (BEFORE creating project folder)
- Research document(s) in `/opt/_research/[project-name]/` or with project files
- Use template: `/opt/fabrik/templates/scaffold/docs/RESEARCH_TEMPLATE.md`

### docs/SERVICES.md (MUST if project has any service)

Document all services the project needs to function:

```markdown
# Required Services

## Services This Project Runs

| Service | Port | Health Endpoint | Watchdog | Purpose |
|---------|------|-----------------|----------|---------|
| API Server | 8000 | /health | scripts/watchdog_api.sh | Main API |
| Worker | - | - | scripts/watchdog_worker.sh | Background jobs |

## External Dependencies

| Service | Required | Purpose | Fallback |
|---------|----------|---------|----------|
| PostgreSQL | Yes | Main database | None |
| Redis | Optional | Caching | Works without, slower |

## Startup Order

1. PostgreSQL (external)
2. Redis (external, optional)
3. API Server: `./scripts/watchdog_api.sh start`
4. Worker: `./scripts/watchdog_worker.sh start`

## Quick Start All Services

\`\`\`bash
./scripts/start_all.sh    # Start everything
./scripts/stop_all.sh     # Stop everything
./scripts/status.sh       # Check all services
\`\`\`
```

Update this file whenever you add/remove/modify a service.

## File Organization Rules

When creating markdown files:
1. Root-level: Only README.md, CHANGELOG.md, tasks.md, AGENTS.md, LICENSE.md, CONTRIBUTING.md
2. All other docs go in docs/ with appropriate subfolder
3. Use UPPERCASE for required docs, lowercase-kebab-case for others
4. Archive old docs to docs/archive/YYYY-MM-DD-topic/

docs/ subfolders:
- guides/ - How-to documentation
- reference/ - Technical specs (API, CLI, database)
- operations/ - Operational procedures
- development/ - Contributor documentation
- archive/ - Obsolete documentation

## Templates

Use templates from: /opt/fabrik/templates/scaffold/docs/
- PROJECT_README_TEMPLATE.md
- DOCS_INDEX_TEMPLATE.md
- CHANGELOG_TEMPLATE.md
- QUICKSTART_TEMPLATE.md
- TROUBLESHOOTING_TEMPLATE.md

## LLM Consultation

To consult Claude or ChatGPT:
- See: /opt/llm_batch_processor/docs/HOW_TO_CONSULT_LLM.md
- API: /opt/llm_batch_processor/llm_batch/api.py

Quick usage:
```python
from llm_batch.api import LLMClient
client = LLMClient(provider="claude", model="sonnet")
response = client.ask("Your question")
client.close()
```

## Migration Workflow

When asked to migrate/update documentation structure:

1. First analyze current state - don't change anything yet
2. Show migration plan with phases:
   - Phase 1: Create missing required files (safe)
   - Phase 2: Create docs/ subfolders (safe)
   - Phase 3: Update docs/README.md index (safe)
   - Phase 4: Move files to proper locations (confirm each)
   - Phase 5: Archive obsolete docs (review list first)
   - Phase 6: Remove duplicates (explicit approval only)
3. Execute only after user approves each phase
4. Verify structure matches standard when complete

## Forbidden Actions

Never:
- Create misc/ folders for docs
- Scatter MD files in project root
- Use todo.md, notes.md, temp.md
- Create deeply nested doc structures
- Leave orphan/unlinked documentation
- **Change code without updating docs** (database schema → update docs/reference/database.md, API → update docs/reference/api.md, config → update docs/CONFIGURATION.md)
- **Complete a task without running self-review checklist**
- **Create a service without a watchdog script**
- **Create a service without documenting it in docs/SERVICES.md**

## Before-Writing-Code Protocol

No code until this checklist is answered:

```
GOAL: [one sentence outcome]
PLAN: [tools/libs chosen; why; key risks]
DEPLOY: [WSL | VPS Docker | Supabase] — Will config work in all targets?
ISSUES: "Known issues for [tool]? If yes, list now."
CONFIRM: "Proceed?" → Wait for explicit "yes"
```

**DEPLOY check examples:**
- ✅ `DB_HOST = os.getenv('DB_HOST', 'localhost')` — Works everywhere
- ❌ `DB_HOST = 'localhost'` — Breaks in Docker/Supabase
- ✅ Health check does `SELECT 1` — Catches DB issues
- ❌ Health check returns `{"status": "ok"}` — Hides failures

Time estimates: "X-Y minutes (Z% confidence)" - AI execution time only.

**If user provides PREPLAN.md:** Read it first, confirm understanding, then proceed.

### Think First (Complex Tasks)

For complex or multi-step tasks:
1. Say: "Let me think through this first before coding..."
2. Outline the approach in plain language
3. Identify potential issues BEFORE writing code
4. Get user approval on approach
5. THEN implement

Don't jump straight to code for anything non-trivial.

## Error Handling Standards

### Error Categories
- `network` - Retry with backoff [5s, 15s, 30s]
- `rate_limit` - Wait 1 hour, then retry
- `critical` - Stop immediately, log CRITICAL
- `permanent` - Skip, log ERROR, don't retry
- `transient` - Retry immediately

For production systems, see: `/opt/fabrik/templates/scaffold/PYTHON_PRODUCTION_STANDARDS.md`

### Resilience Requirements
- Network monitor: Check connectivity before operations
- Recovery on startup: Resume incomplete tasks
- Atomic writes: Use temp file + rename pattern
- Retry queue: Persist failed tasks for later

## Python Script Template

Every Python script must include:
```python
"""
Script: name.py
Purpose: One line description
Version: X.Y.Z
"""

# Feature flags (kill switches)
ENABLE_RETRIES = True
DRY_RUN_MODE = False

# Config (no hardcoded values)
from pathlib import Path
import os

BASE_DIR = Path(os.environ.get('APP_BASE_DIR', '/opt/project'))
LOG_FILE = BASE_DIR / 'logs' / 'app.log'

# Logging setup
import logging
from logging.handlers import RotatingFileHandler

logger = logging.getLogger(__name__)
handler = RotatingFileHandler(LOG_FILE, maxBytes=10*1024*1024, backupCount=3)
logger.addHandler(handler)
```

## Database Policy

Default: PostgreSQL 16. One DB per project.

```bash
# Create database
sudo -u postgres psql -c "CREATE DATABASE <project>;"

# Daily backup (add to cron)
pg_dump <db> > /opt/backups/<db>_$(date +%F).sql
```

Code pattern: psycopg2-binary, parameterized queries, connection pooling.

## Security Checklist

Before any deployment:
- [ ] No secrets in code (use .env)
- [ ] .gitignore includes .env, logs/, data/
- [ ] File permissions correct (644 config, 755 scripts)
- [ ] Input validation on all user inputs
- [ ] No command injection vulnerabilities
- [ ] Logs don't contain passwords/tokens
- [ ] Dry-run mode tested

## Technology Defaults

| Need | Default Choice | Upgrade When |
|------|----------------|--------------|
| API | FastAPI + Pydantic | >1k RPS |
| DB | PostgreSQL 16 | >100GB, multi-region |
| Cache | Redis | Hot paths >10ms |
| Jobs | APScheduler/RQ | Many concurrent |
| Scraping | Playwright + httpx | Anti-bot scale |
| Logging | Loki + Promtail | Multi-host |
| Monitoring | Prometheus + Grafana | On-call needed |

## AI Project Selection

For AI tasks, identify category first:
1. Speech/Audio → Soniox, Whisper
2. Vision → YOLO, Midjourney
3. Language → Claude, GPT
4. Code → Copilot, Cursor

Don't use general LLM for specialized tasks (e.g., GPT for transcription).

For full AI taxonomy (15 categories), see: `/opt/fabrik/docs/reference/AI_TAXONOMY.md`

## Complexity Templates

### Simple (single file, <200 lines)
- `project-create-quick name "desc"`
- Basic logging, no external deps

### Medium (2-5 files, <1000 lines)
- Config management, structured logging
- 1-2 external dependencies

### Complex (multi-module, >1000 lines)
- Database, multiple deps, error recovery
- Production logging, migrations

## Task Tracking (MUST)

Every project MUST have `tasks.md`:

```markdown
# Project: [name]

## Phase 1: Setup
- [x] Task 1.1: Initialize project structure
  - [x] Create folders
  - [x] Setup .env.example
- [ ] Task 1.2: Database schema
  - [ ] Design tables
  - [ ] Create migrations

## Phase 2: Core Features
- [ ] Task 2.1: User authentication
  - [ ] Login endpoint
  - [ ] Token validation
```

Rules:
- Update tasks.md after completing each task
- Never work on tasks out of order without approval
- Use for resuming work in new sessions

## Keep Documentation in Sync

When you change code that affects:
- **Database schema** → Update `docs/reference/database.md`
- **API endpoints** → Update `docs/reference/api.md`
- **Configuration** → Update `docs/CONFIGURATION.md` and `.env.example`
- **Dependencies** → Update `requirements.txt` / `package.json`

Don't wait until end - update docs AS you make changes.

### Documentation Structure Map (MUST)

Every project's `docs/README.md` MUST show the current documentation structure:

```markdown
# Project Documentation

## Structure
docs/
├── README.md                    # This file - documentation index
├── QUICKSTART.md                # Get running in 5 minutes
├── CONFIGURATION.md             # All settings reference
├── TROUBLESHOOTING.md           # Common issues & solutions
├── guides/
│   └── integration-guide.md     # How to integrate
└── reference/
    ├── database.md              # Database schema
    └── api.md                   # API endpoints

## Quick Links
- [Getting Started](QUICKSTART.md)
- [Configuration](CONFIGURATION.md)
...
```

**Rules:**
- Update `docs/README.md` structure map whenever you add/remove/rename docs
- Include one-line description for each doc
- Keep structure map at TOP of docs/README.md

## Self-Review Before Done (MUST)

Before marking any task complete, review your own work:

```
SELF-CHECK (ALL must be YES):
- [ ] Does it handle edge cases? (empty input, null, errors)
- [ ] Any hardcoded values that should be config?
- [ ] Security issues? (exposed secrets, injection, etc.)
- [ ] Will it break existing functionality?
- [ ] Is the code readable without comments?
- [ ] DOCS UPDATED? (schema→database.md, API→api.md, config→CONFIGURATION.md)
```

**If any check fails → FIX BEFORE MARKING DONE. No exceptions.**

## When Stuck Protocol

If stuck for >15 minutes on same issue:

1. **Stop and document** the problem clearly
2. **Try 2 different approaches** before escalating
3. **If still stuck**: Consult external AI (Claude/ChatGPT) via:
   ```python
   from llm_batch.api import LLMClient
   client = LLMClient(provider="claude", model="sonnet")
   response = client.ask("I'm stuck on: [problem]. Tried: [approaches]. Error: [details]")
   ```
4. Apply solution and document what worked

Don't spin wheels - escalate early, solve fast.

## Continue Project Protocol

When resuming work:
1. Read `tasks.md` + recent git log
2. Show: current phase, next task, blockers if any
3. Ask: "Proceed with [next task]?"
4. Wait for approval before any changes

## Command Execution Strategy

### Tier Classification

| Tier | Expected Duration | Method | Examples |
|------|-------------------|--------|----------|
| A | <30s | Run normally (Blocking=true) | `ls`, `cat`, `git status`, `npm run lint` |
| B | 30s-2min | Native non-blocking OR Tier C | `npm test`, `pytest`, small builds |
| C | >2min or complex | `rund`/`rundsh` + `runc` monitoring | `npm install`, `docker build`, migrations |

### Tier A - Fast Commands (<30s)

Run with `Blocking=true`. Examples: ls, cat, grep, git status, git add, git commit, echo, pwd, npm run lint

### Tier C - Long/Complex Commands (Always use rund/rundsh)

Use for ANY of:
- `npm install` / `npm ci`
- `pip install` (multiple packages)
- `docker build` / `docker pull` / `docker-compose up`
- `git clone` (large repos)
- `cargo build` / `go build` / `make` / `mvn` / `gradle`
- Database migrations
- Any network-dependent operation
- Anything that has hung before

### Long Command Tools (rund/rundsh/runc/runk)

Scripts location: `/opt/fabrik/scripts/` (symlinked to `~/.local/bin/`)

```bash
rund <cmd> [args]       # exec mode - argv safe, no shell interpretation
rundsh '<shell line>'   # shell mode - pipes/redirects allowed, TRUSTED INPUT ONLY
runc <job_path>         # check job status
runk <job_path>         # kill stuck job (SID-based, kills process tree)
```

### Monitoring Workflow (Tier C)

```
1. Start:  rund npm install
   Output: JOB=/tmp/job_xxx PID=123 SID=123

2. Every 15s: runc /tmp/job_xxx
   Record: LOG bytes, cputime, PROCS

3. If DONE → report exit code, proceed
   If RUNNING + progressing → continue monitoring
   If STUCK 90s → runk, retry with --verbose/--progress
```

### Stuck Detection

STUCK = 90 seconds of ALL true:
- LOG size unchanged
- CPU time unchanged
- Process count unchanged
- NOT showing expected wait (retry, backoff, "waiting for...")

### runc Output Reference

| State | Meaning |
|-------|---------|
| `RUNNING` | Active - check LOG/cputime/PROCS for progress |
| `DONE exit=0` | Success |
| `DONE exit=N` | Failed with code N |
| `KILLED` | Terminated by signal |
| `DEAD (no rc)` | Crashed without writing return code |

### Quick Reference

```
┌─────────────────────────────────────────────────────────────┐
│ LONG COMMAND MONITORING                                     │
├─────────────────────────────────────────────────────────────┤
│ START:    rund npm install                                  │
│ MONITOR:  runc /tmp/job_xxx     (every 15s)                │
│ KILL:     runk /tmp/job_xxx     (if stuck 90s)             │
│                                                             │
│ STUCK = 90s of: LOG same + cputime same + PROCS same       │
│ WHEN IN DOUBT → USE TIER C (rund/rundsh)                   │
└─────────────────────────────────────────────────────────────┘
```

### Port Management (MUST)

**Before starting ANY dev server or assigning ports:**

1. **Check `/opt/fabrik/data/ports.yaml`** for conflicts
2. **Check system ports** with `ss -tlnp | grep <port>` (Ubuntu services like Tomcat may use common ports)
3. **Register your port** in PORTS.md before using it
4. **Update project's `docs/DEPLOYMENT.md`** with port info

```bash
# Check if port is available
ss -tlnp | grep 8002 || echo "Port 8002 is free"

# After confirming, register in PORTS.md:
# | 8002 | my-project | FastAPI | TCP | My Project API |
```

**Port ranges (convention):**
- 8000-8099: Python APIs (FastAPI/Flask)
- 8100-8199: Workers/Background services
- 3000-3099: Frontend apps (Next.js/React)
- 5000-5099: Internal tools

**Rules:**
- **Never hardcode ports** — use environment variables
- **Never grab a port without checking** — conflicts waste everyone's time
- **System services have priority** — don't fight with Tomcat, Apache, nginx

### Servers/Daemons

Always non-blocking with health check:

```bash
# Start server non-blocking, then verify
python server.py &
sleep 2 && curl -s localhost:8000/health
```

### Service Watchdog (MUST)

**Any service that should stay running MUST have a watchdog.**

When you create a service/daemon/API that needs to stay up:

1. **Create the service** (Python, Node, etc.)
2. **Create health endpoint** (`/health` or `/ping`)
3. **Create watchdog script** in `scripts/watchdog_<service>.sh`
4. **Document in README** how to start with watchdog

#### Watchdog Template

Create `scripts/watchdog_<service>.sh`:

```bash
#!/bin/bash
# Watchdog for <service_name>
# Usage: ./watchdog_<service>.sh start|stop|status

SERVICE_NAME="<service_name>"
HEALTH_URL="http://localhost:<port>/health"
START_CMD="python src/server.py"  # or your start command
PID_FILE="/tmp/${SERVICE_NAME}.pid"
LOG_FILE="logs/${SERVICE_NAME}.log"
CHECK_INTERVAL=30

start_service() {
    if [ -f "$PID_FILE" ] && kill -0 $(cat "$PID_FILE") 2>/dev/null; then
        echo "$SERVICE_NAME already running"
        return
    fi
    mkdir -p logs
    nohup $START_CMD >> "$LOG_FILE" 2>&1 &
    echo $! > "$PID_FILE"
    echo "$SERVICE_NAME started (PID: $(cat $PID_FILE))"
}

check_health() {
    curl -sf "$HEALTH_URL" > /dev/null 2>&1
}

watchdog_loop() {
    while true; do
        if ! check_health; then
            echo "$(date): $SERVICE_NAME unhealthy, restarting..."
            stop_service 2>/dev/null
            sleep 2
            start_service
        fi
        sleep $CHECK_INTERVAL
    done
}

stop_service() {
    if [ -f "$PID_FILE" ]; then
        kill $(cat "$PID_FILE") 2>/dev/null
        rm -f "$PID_FILE"
        echo "$SERVICE_NAME stopped"
    fi
}

case "$1" in
    start) start_service && watchdog_loop ;;
    stop) stop_service ;;
    status) check_health && echo "UP" || echo "DOWN" ;;
    *) echo "Usage: $0 {start|stop|status}" ;;
esac
```

#### Quick Watchdog Options

| Method | Best For | Command |
|--------|----------|---------|
| Bash script | Simple services | `./scripts/watchdog_api.sh start` |
| systemd | Production Linux | Create `.service` file |
| pm2 | Node.js | `pm2 start server.js --watch` |
| supervisord | Multiple services | Add to supervisord.conf |

#### Minimum Requirements

- [ ] Health endpoint exists (`/health` returns 200)
- [ ] Watchdog script created
- [ ] Start command documented in README
- [ ] Logs go to `logs/` folder

## Git Requirements (MUST)

Every project MUST be a git repository. No exceptions.

### Initial Setup

```bash
# Initialize git BEFORE writing any code
git init
git add .
git commit -m "Initial commit: project structure"
```

### Commit Rules

- First commit immediately after creating project structure
- Small, focused commits - one logical change per commit
- Commit messages: `type: brief description`
  - Types: feat, fix, docs, refactor, test, chore
  - Example: `feat: add user authentication endpoint`
- Never commit: `.env`, secrets, `node_modules/`, `venv/`, `__pycache__/`

### Branch Strategy (Simple)

- `main` - production-ready code
- Feature branches: `feature/short-name`
- Fix branches: `fix/issue-description`

Merge to main only when:
- All tests pass
- Lint/typecheck clean
- Code reviewed (self-review minimum)

## Pre-commit Hooks (Python Projects)

### Setup (Required for Python Projects)

```bash
# Install pre-commit
pip install pre-commit --break-system-packages

# Copy config template
cp /opt/fabrik/templates/scaffold/pre-commit-config.yaml .pre-commit-config.yaml

# Install hooks
pre-commit install

# Verify installation
pre-commit run --all-files
```

### What Pre-commit Enforces

| Hook | Purpose | Blocks Commit If |
|------|---------|------------------|
| ruff | Linting + formatting | Code style violations |
| mypy | Type checking | Type errors |
| trailing-whitespace | Clean files | Trailing spaces |
| check-yaml | Valid configs | Invalid YAML |
| detect-private-key | Security | Secrets in code |
| bandit | Security scan | Security issues |

### Skipping Hooks (Emergency Only)

```bash
# Skip all hooks (use sparingly)
git commit --no-verify -m "emergency fix"

# Skip specific hook
SKIP=mypy git commit -m "fix: urgent patch"
```

Document WHY you skipped in the commit message.

### Pre-commit Config Location

Template: `/opt/fabrik/templates/scaffold/pre-commit-config.yaml`

Minimal version (for faster commits):
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.8.0
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
```

## Python Project Dependencies (MUST)

### Required Files

Every Python project MUST have:

```
project/
├── .python-version          # Python version (e.g., "3.12")
├── pyproject.toml           # Project config + dependencies
├── requirements.lock        # Locked dependencies (or uv.lock)
└── .pre-commit-config.yaml  # Pre-commit hooks
```

### Dependency Management (Use uv)

```bash
# Initialize new project
uv init

# Add dependencies
uv add fastapi pydantic

# Add dev dependencies
uv add --dev pytest ruff mypy

# Lock dependencies
uv lock

# Install from lock file
uv sync
```

### pyproject.toml Template

Use template: `/opt/fabrik/templates/scaffold/python/pyproject.toml.template`

### .python-version File

Create in project root:
```
3.12
```

This ensures consistent Python version across environments.

## Quality Gate (MUST Pass Before Done)

No task is complete until:

```bash
# Run quality checks
ruff check .                    # Lint
ruff format --check .           # Format check
mypy .                          # Type check
pytest                          # Tests

# Or use pre-commit to run all
pre-commit run --all-files
```

### Integration with Self-Review

Update self-review checklist to include:

```
SELF-CHECK (ALL must be YES):
- [ ] `ruff check .` passes
- [ ] `mypy .` passes (or `# type: ignore` documented)
- [ ] `pytest` passes
- [ ] Pre-commit hooks pass
- [ ] Git commit made with meaningful message
```

## Container-First Development

### Core Principle

**Container-first but not container-only.**

- Write code with normal local tooling (venv/npm) for **speed**
- Keep a **working Dockerfile + compose** from day 1
- Use Docker as the **truth for production parity**

This avoids two failure modes:

| Failure | Solution |
|---------|----------|
| "Works locally but not in container" | Docker-ready from day 1, `docker build` as gate |
| "Container works but dev is slow" | Local venv/npm for speed, Docker only for parity |

### Development Workflow

```text
LOCAL DEV (fast):     uv venv + uvicorn --reload  OR  npm run dev
DOCKER PARITY (gate): make docker-smoke (build + run + health check)
DEPLOY:               git push → Coolify auto-deploys
```

---

## Deployment-Ready Compliance (VPS via Coolify)

### Trigger: "comply with windsurfrules"

When user says **"comply with windsurfrules"** in any project folder, run the full compliance check below. This prepares the project for VPS deployment via Coolify.

### Compliance Checklist

Run this checklist and fix any missing items:

```text
DEPLOYMENT-READY CHECK:
- [ ] Dockerfile exists (Python or Node.js template)
- [ ] compose.yaml exists (production-like, no bind mounts)
- [ ] compose.dev.yaml exists (optional, dev overrides)
- [ ] .dockerignore exists (excludes venv, node_modules, .env)
- [ ] .env.example exists and documents ALL required env vars
- [ ] Makefile exists with: make dev, make docker-smoke, make test
- [ ] Health check endpoint exists (GET /health returns {"status": "ok"})
- [ ] App binds to 0.0.0.0 (not localhost/127.0.0.1)
- [ ] Port configurable via environment variable
- [ ] Migrations strategy defined (Alembic/Prisma)
- [ ] README.md has deployment section
- [ ] Git repository initialized with .gitignore
```

### Required Files for Deployment

Every deployable project MUST have:

```
/opt/<project>/
├── Dockerfile              # Container build instructions
├── compose.yaml            # Coolify deployment config
├── .dockerignore           # Exclude unnecessary files
├── .env.example            # Document required env vars
├── README.md               # Include deployment section
└── docs/DEPLOYMENT.md      # Full deployment documentation
```

### Dockerfile Templates

**Python FastAPI** (`Dockerfile`):
```dockerfile
FROM python:3.12-slim AS builder
WORKDIR /app
RUN apt-get update && apt-get install -y --no-install-recommends gcc libpq-dev && rm -rf /var/lib/apt/lists/*
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

FROM python:3.12-slim
WORKDIR /app
RUN apt-get update && apt-get install -y --no-install-recommends libpq5 curl && rm -rf /var/lib/apt/lists/*
COPY --from=builder /root/.local /root/.local
ENV PATH=/root/.local/bin:$PATH
COPY . .
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:${PORT:-8000}/health || exit 1
ENV PORT=8000
EXPOSE ${PORT}
CMD ["sh", "-c", "uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}"]
```

**Node.js** (`Dockerfile`):
```dockerfile
FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build --if-present

FROM node:20-alpine
WORKDIR /app
RUN apk add --no-cache curl
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/package*.json ./
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:${PORT:-3000}/health || exit 1
ENV PORT=3000
EXPOSE ${PORT}
CMD ["node", "dist/index.js"]
```

### compose.yaml Template (Coolify-compatible)

```yaml
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "${PORT:-8000}:${PORT:-8000}"
    environment:
      - PORT=${PORT:-8000}
      - DATABASE_URL=${DATABASE_URL:?Database URL is required}
      # Add all required env vars with :? for validation
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
```

### .dockerignore Template

```
venv/
.venv/
node_modules/
.env
.git/
__pycache__/
*.py[cod]
logs/
data/
.tmp/
.cache/
tests/
docs/
*.md
```

### Health Check Endpoint (MUST)

**Health checks MUST verify actual dependencies, not just return OK.**

**Python FastAPI (CORRECT - tests DB connection):**
```python
@app.get("/health")
async def health():
    try:
        # Actually test the database connection
        async with db.acquire() as conn:
            await conn.execute("SELECT 1")
        return {"status": "ok", "database": "connected"}
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Database unhealthy: {e}")
```

**Python FastAPI (WRONG - shallow check that hides failures):**
```python
@app.get("/health")
async def health():
    return {"status": "ok"}  # NEVER DO THIS - hides real problems
```

**Node.js Express/Fastify:**
```javascript
app.get('/health', (req, res) => res.json({ status: 'ok' }));
```

### Host Binding (MUST)

App MUST bind to `0.0.0.0` for Docker networking:

**Python (uvicorn):**
```python
uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
```

**Node.js (Express):**
```javascript
app.listen(PORT, '0.0.0.0', () => console.log(`Listening on ${PORT}`));
```

### Compliance Workflow

When "comply with windsurfrules" is triggered:

1. **Scan project** - Identify what exists vs what's missing
2. **Report status** - Show checklist with ✅/❌ for each item
3. **Fix missing items** - Create files from templates above
4. **Verify** - Run `docker build -t <project> .` to test
5. **Commit** - `git add . && git commit -m "feat: add Docker deployment configuration"`

### Development Iteration Flow (WSL → VPS)

```
WSL (Development)              GitHub                 Coolify (VPS)
     │                            │                        │
     ├── Edit code ───────────────┼────────────────────────┤
     ├── Test locally             │                        │
     ├── git push ────────────────┼──► Webhook ────────────┤
     │                            │                        ├── Pull code
     │                            │                        ├── Build image
     │                            │                        ├── Deploy container
     │                            │                        ▼
     │                            │                   Running service
```

Every `git push` can trigger automatic redeploy in Coolify.

### Reference Documentation

- Full checklist: `/opt/fabrik/docs/DEPLOYMENT_READY_CHECKLIST.md`
- Project registry: `/opt/fabrik/docs/reference/project-registry.md`
- Stack reference: `/opt/fabrik/docs/reference/stack.md`

---

## Fabrik Microservice Convention

When building microservices that Fabrik will deploy and WordPress sites will consume:

### Requirements

| Requirement | Details |
|-------------|---------|
| **Docker** | `compose.yaml` + `Dockerfile` required |
| **Health** | `GET /health` endpoint (Coolify monitors this) |
| **URL Pattern** | `https://service.vps1.ocoron.com` |
| **Env Var** | Add `SERVICE_URL` to `/opt/fabrik/.env` |
| **Credentials** | Store API keys in both project `.env` AND `/opt/fabrik/.env` |

### Service URL Convention

```bash
# Pattern: https://<service>.vps1.ocoron.com
IMAGE_BROKER_URL=https://images.vps1.ocoron.com
PROXY_API_URL=https://proxy.vps1.ocoron.com
TRANSLATOR_API_URL=https://translator.vps1.ocoron.com
```

### Integration with WordPress

WordPress sites call microservices via environment variables:
```php
$image_broker_url = getenv('IMAGE_BROKER_URL');
$response = wp_remote_get("$image_broker_url/api/v1/search?query=business");
```

### Full Guide

See: `/opt/fabrik/docs/guides/FABRIK_INTEGRATION.md`

---

## Database Strategy Reference

For database environment selection, migrations, and vector storage:
See: `/opt/fabrik/docs/reference/DATABASE_STRATEGY.md`

Quick reference:
- Local dev: PostgreSQL (WSL)
- Production small: PostgreSQL (VPS)
- Production scale: Supabase
- Vectors: pgvector (built into PostgreSQL/Supabase)

Migration policy: **Forward-only**. Never edit existing migrations.
